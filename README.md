# ğŸ” Airflow Sqoop-Hive ETL Pipeline

A lightweight ETL pipeline that extracts data from a local **MySQL** database, transfers it to **HDFS** using **Sqoop**, loads it into **Hive**, and validates the data â€” all orchestrated using **Apache Airflow**.

---
<div align="center">
    <img src="https://github.com/AdhamAymanElsayed/airflow-sqoop-hive-pipeline/blob/main/pipeline.png?raw=true" alt="Airflow Sqoop Hive Pipeline" width="700"/>
</div>
---

## âš™ï¸ Stack

- ğŸ¬ **MySQL** â€“ Source database (host machine)
- ğŸ”„ **Sqoop** â€“ Data import into HDFS
- ğŸ˜ **HDFS** & ğŸ **Hive** â€“ Storage & querying (Hortonworks VM)
- ğŸ§© **Apache Airflow** â€“ DAG orchestration (Docker)

---

## ğŸ§­ Workflow Steps

1. **Sqoop Import** â€“ Extract MySQL data to HDFS  
2. **Create Hive Table** â€“ Define table schema  
3. **Load into Hive** â€“ Move data from HDFS  
4. **Validate** â€“ Count records in Hive

---

## ğŸ—ºï¸ DAG Structure

```mermaid
graph TD
    Start([ğŸš€ Start]) --> Sqoop([ğŸ”„ Sqoop Import])
    Sqoop --> Create([ğŸ“Š Create Hive Table])
    Create --> Load([ğŸ“¥ Load Data to Hive])
    Load --> Validate([ğŸ” Validate Data])
    Validate --> End([âœ… End])
